{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kshk8f5rzCrK"
   },
   "source": [
    "# Visual Language Models (VLMs) Lab Exercise\n",
    "## ITAI 1378 - Module 08\n",
    "\n",
    "---\n",
    "\n",
    "### Lab Overview\n",
    "\n",
    "Welcome to the VLM Lab! In this hands-on exercise, you'll explore the fascinating world of Visual Language Models - AI systems that can understand and reason about both images and text.\n",
    "\n",
    "**Estimated Time:** 2-2.5 hours\n",
    "\n",
    "### Learning Outcomes\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. **Understand VLMs and Their Architectures**: Explain how VLMs work and differentiate between major architectural approaches (CLIP, BLIP, LLaVA)\n",
    "2. **Understand Applications of VLMs**: Identify and implement practical applications like image search, captioning, and visual question answering\n",
    "3. **Evaluate and Apply**: Analyze trade-offs between different VLM approaches and make informed decisions for real-world problems\n",
    "\n",
    "### Two Paths Available\n",
    "\n",
    "This lab provides **two implementation paths** based on your available compute resources:\n",
    "\n",
    "- **üöÄ Path A (Limited Compute)**: Uses CLIP and smaller models - runs on CPU or basic GPU\n",
    "- **‚ö° Path B (More Resources)**: Uses BLIP-2 and larger models - requires GPU with 8GB+ VRAM\n",
    "\n",
    "**You only need to complete ONE path**, but you're welcome to explore both!\n",
    "\n",
    "---\n",
    "\n",
    "### ü§ñ Using AI Assistants Effectively\n",
    "\n",
    "**You are encouraged to use AI assistants (ChatGPT, Claude, etc.) for coding help!** However, use them wisely:\n",
    "\n",
    "‚úÖ **Good uses:**\n",
    "- Understanding error messages\n",
    "- Learning Python syntax and concepts\n",
    "- Debugging code that isn't working\n",
    "- Explaining what a code block does\n",
    "\n",
    "‚ùå **Avoid:**\n",
    "- Copy-pasting reflection questions into AI without thinking first\n",
    "- Having AI write your conceptual answers\n",
    "- Using AI to skip the learning process\n",
    "\n",
    "**Remember**: The reflection questions are designed to make you think critically. Write your own thoughts first, then optionally discuss with AI to refine your understanding.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nt0rXWDpzCrL"
   },
   "source": [
    "## Section 0: Environment Setup\n",
    "\n",
    "Let's start by setting up our environment and installing the necessary libraries.\n",
    "\n",
    "### üìö For Python Beginners\n",
    "\n",
    "- **Libraries/Packages**: Collections of pre-written code that add functionality\n",
    "- **pip**: Python's package installer (like an app store for code)\n",
    "- **!** in Jupyter: Runs a command in the terminal, not in Python\n",
    "- **import**: Brings a library's functionality into your code\n",
    "\n",
    "**üí° Tip**: If installation fails, try restarting the kernel (Kernel ‚Üí Restart) and running again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "iu55BkvNzCrL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Installation complete!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# This might take 3-5 minutes - be patient!\n",
    "\n",
    "!pip install -q torch torchvision transformers pillow matplotlib datasets\n",
    "!pip install -q ftfy regex tqdm\n",
    "!pip install -q sentencepiece  # For some models\n",
    "\n",
    "\n",
    "\n",
    "print(\"‚úì Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhFrHCRmzCrL"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, BlipForQuestionAnswering\n",
    "from datasets import load_dataset\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"Running on CPU - consider using Path A (Limited Compute)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9WDtPA-zCrL"
   },
   "source": [
    "### ‚úÖ Knowledge Check 1: Environment\n",
    "\n",
    "**Question**: What is the purpose of checking for GPU availability in the code above? Why might some VLM operations be slow on CPU?\n",
    "\n",
    "**Your Answer**:\n",
    "Checking GPU availability lets the notebook choose the right device (CUDA GPU vs. CPU) for tensor operations. VLMs rely on large matrix multiplications and attention computations over high‚Äëdimensional tensors; GPUs are optimized for massively parallel math and high memory bandwidth, so inference/training is typically much faster. On CPU, these same operations run with far less parallelism and lower throughput, which can make model loading and generation/classification noticeably slow.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_ihQK1czCrL"
   },
   "source": [
    "## Section 1: Understanding VLM Fundamentals\n",
    "\n",
    "Before we dive into code, let's explore the core concepts that make VLMs work.\n",
    "\n",
    "### üéØ Recall: The Three Building Blocks\n",
    "\n",
    "Every VLM consists of three core components:\n",
    "\n",
    "1. **Vision Encoder** üñºÔ∏è - Converts images into numerical representations (embeddings)\n",
    "2. **Language Model** üß† - Understands and generates text\n",
    "3. **The Bridge** üåâ - Connects vision and language (via cross-attention or projection)\n",
    "\n",
    "### The Magic of Shared Embedding Space\n",
    "\n",
    "The key insight: **Both images and text are converted into vectors in the same high-dimensional space**. This allows us to:\n",
    "\n",
    "- Measure how similar an image is to a text description\n",
    "- Find images that match a text query\n",
    "- Generate text descriptions of images\n",
    "\n",
    "Let's visualize this concept!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WB14UtvczCrL"
   },
   "outputs": [],
   "source": [
    "# Helper function to load images from URLs\n",
    "def load_image_from_url(url):\n",
    "    \"\"\"Load an image from a URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    return img\n",
    "\n",
    "# Helper function to display images\n",
    "def show_images(images, titles=None, cols=3):\n",
    "    \"\"\"Display multiple images in a grid.\"\"\"\n",
    "    n_images = len(images)\n",
    "    rows = (n_images + cols - 1) // cols\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
    "    if rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    for idx, img in enumerate(images):\n",
    "        row = idx // cols\n",
    "        col = idx % cols\n",
    "        axes[row, col].imshow(img)\n",
    "        axes[row, col].axis('off')\n",
    "        if titles and idx < len(titles):\n",
    "            axes[row, col].set_title(titles[idx], fontsize=12, wrap=True)\n",
    "\n",
    "    # Hide empty subplots\n",
    "    for idx in range(n_images, rows * cols):\n",
    "        row = idx // cols\n",
    "        col = idx % cols\n",
    "        axes[row, col].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úì Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrZRQjjPzCrL"
   },
   "source": [
    "### ü§î Reflection Question 1: The Alignment Problem\n",
    "\n",
    "In the lecture, we discussed the \"alignment problem\" - the challenge of creating a shared representation space where visual and textual concepts are aligned.\n",
    "\n",
    "**Question**: Why is it challenging to align vision and language? Think about:\n",
    "- How images and text are fundamentally different\n",
    "- What makes the word \"cat\" similar to a picture of a cat in a meaningful way\n",
    "- What information might be lost or gained when converting each modality to embeddings\n",
    "\n",
    "**Your Answer**:\n",
    "Aligning vision and language is difficult because images and text encode meaning in fundamentally different ways. Images are continuous, high‚Äëdimensional signals with spatial structure (multiple objects, attributes, and relationships), while text is discrete and sequential, often describing only part of what is visible and at varying levels of specificity. To make the word ‚Äúcat‚Äù land near a picture of a cat, the model must learn robust correspondences across many visual variations (pose, lighting, background, breed) and many textual phrasings (synonyms, context). Paired data can be noisy or ambiguous, and compressing both modalities into fixed-length embeddings inevitably loses some detail (e.g., exact counts or small text), so the model must preserve shared semantics while becoming invariant to irrelevant pixel-level differences.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_arX5WiUzCrL"
   },
   "source": [
    "## Section 2: Exploring VLM Architectures\n",
    "\n",
    "Now let's explore the different architectural approaches discussed in the lecture.\n",
    "\n",
    "### Architecture Comparison (From Lecture)\n",
    "\n",
    "| Architecture | Approach | Key Feature | Best For |\n",
    "|-------------|----------|-------------|----------|\n",
    "| CLIP | Contrastive | Zero-shot classification | Simple classification, search |\n",
    "| BLIP | Projector + Gen | Bootstrapping data | VQA, Captioning |\n",
    "| LLaVA | Projector | Instruction tuning | Multimodal chat, reasoning |\n",
    "| Flamingo | Adapter | Few-shot learning | Tasks with limited data |\n",
    "| GPT-4V | Integrated | State-of-the-art | Complex reasoning, analysis |\n",
    "\n",
    "### Choose Your Path!\n",
    "\n",
    "- **Path A (Limited Compute)**: We'll use **CLIP** - excellent for zero-shot tasks, runs on CPU\n",
    "- **Path B (More Resources)**: We'll use **BLIP-2** - better for generation tasks, needs GPU\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXZpZCr8zCrM"
   },
   "source": [
    "## üöÄ PATH A: Limited Compute (CLIP)\n",
    "\n",
    "### Introduction to CLIP\n",
    "\n",
    "**CLIP (Contrastive Language-Image Pre-training)** was developed by OpenAI and trained on 400 million image-text pairs from the internet.\n",
    "\n",
    "**Key Innovation**: Contrastive learning\n",
    "- Given a batch of images and text descriptions\n",
    "- Learn to match correct pairs (positive examples)\n",
    "- Push apart incorrect pairs (negative examples)\n",
    "\n",
    "**Result**: A shared embedding space where similar images and text are close together!\n",
    "\n",
    "### Applications of CLIP\n",
    "1. Zero-shot image classification\n",
    "2. Image-text similarity/search\n",
    "3. Foundation for other models (like Stable Diffusion)\n",
    "\n",
    "Let's load CLIP and explore its capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Au2C-aAMzCrM"
   },
   "outputs": [],
   "source": [
    "# PATH A: Load CLIP model (runs on CPU)\n",
    "print(\"Loading CLIP model... (this may take 1-2 minutes)\")\n",
    "\n",
    "model_name = \"openai/clip-vit-base-patch32\"  # Smaller, faster version\n",
    "clip_model = CLIPModel.from_pretrained(model_name)\n",
    "clip_processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Move to device (GPU if available, CPU otherwise)\n",
    "clip_model = clip_model.to(device)\n",
    "\n",
    "print(\"‚úì CLIP model loaded successfully!\")\n",
    "print(f\"Model size: ~600MB\")\n",
    "print(f\"Embedding dimension: {clip_model.config.projection_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzyjusbRzCrM"
   },
   "source": [
    "### üìö Understanding the Code\n",
    "\n",
    "- **`from_pretrained()`**: Loads a pre-trained model (saved weights) instead of training from scratch\n",
    "- **Processor**: Handles pre-processing (resizing images, tokenizing text) to match model's expected input format\n",
    "- **`.to(device)`**: Moves model to GPU (if available) or keeps on CPU\n",
    "- **Embedding dimension**: The size of the vector representation (higher = more expressive, but slower)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGjw_u1azCrM"
   },
   "source": [
    "### Experiment 1: Zero-Shot Image Classification\n",
    "\n",
    "Let's test CLIP's zero-shot classification capability. We'll classify images into categories that CLIP has never explicitly been trained to recognize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OD3bbzaIzCrM"
   },
   "outputs": [],
   "source": [
    "# Load a sample image from COCO dataset\n",
    "print(\"Loading sample images from COCO dataset...\")\n",
    "\n",
    "# Load COCO validation dataset (small subset)\n",
    "coco_dataset = load_dataset(\"yerevann/coco-karpathy\", split=\"validation[:20]\")  # Just 20 images for speed\n",
    "\n",
    "# Print the keys available in the dataset object to diagnose the issue\n",
    "print(\"Keys available in the dataset:\")\n",
    "print(coco_dataset.features.keys())\n",
    "\n",
    "# Get a random image index\n",
    "import random\n",
    "random_index = random.randint(0, len(coco_dataset) - 1)\n",
    "\n",
    "# Get the image URL instead of filepath\n",
    "image_url = coco_dataset[random_index]['url']\n",
    "\n",
    "print(f\"Downloading image from: {image_url}\")\n",
    "\n",
    "# Download and open the image from URL\n",
    "response = requests.get(image_url)\n",
    "sample_image = Image.open(BytesIO(response.content))\n",
    "\n",
    "print(f\"Image loaded! Size: {sample_image.size}\")\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(sample_image)\n",
    "plt.axis('off')\n",
    "plt.title(\"Sample Image from COCO\")\n",
    "plt.show()\n",
    "\n",
    "# Fix: sentences might be a list of dicts or a different structure\n",
    "sentences = coco_dataset[random_index]['sentences']\n",
    "if isinstance(sentences, list) and len(sentences) > 0:\n",
    "    if isinstance(sentences[0], dict):\n",
    "        print(f\"Caption: {sentences[0]['raw']}\")\n",
    "    else:\n",
    "        print(f\"Caption: {sentences[0]}\")\n",
    "else:\n",
    "    print(f\"Caption: {sentences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLuaXyu3zCrM"
   },
   "outputs": [],
   "source": [
    "# Zero-shot classification with CLIP\n",
    "def classify_image_clip(image, text_labels):\n",
    "    \"\"\"\n",
    "    Classify an image using CLIP with provided text labels.\n",
    "\n",
    "    Args:\n",
    "        image: PIL Image\n",
    "        text_labels: List of text descriptions (categories)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with probabilities for each label\n",
    "    \"\"\"\n",
    "    # Process inputs\n",
    "    inputs = clip_processor(\n",
    "        text=text_labels,\n",
    "        images=image,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():  # Don't compute gradients (faster, uses less memory)\n",
    "        outputs = clip_model(**inputs)\n",
    "\n",
    "    # Calculate similarity scores (logits)\n",
    "    logits_per_image = outputs.logits_per_image\n",
    "\n",
    "    # Convert to probabilities\n",
    "    probs = logits_per_image.softmax(dim=1).cpu().numpy()[0]\n",
    "\n",
    "    # Create results dictionary\n",
    "    results = {label: prob for label, prob in zip(text_labels, probs)}\n",
    "    return results\n",
    "\n",
    "# Test with different categories\n",
    "categories = [\n",
    "    \"a photo of a dog\",\n",
    "    \"a photo of a cat\",\n",
    "    \"a photo of a person\",\n",
    "    \"a photo of food\",\n",
    "    \"a photo of a vehicle\"\n",
    "]\n",
    "\n",
    "results = classify_image_clip(sample_image, categories)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n=== Classification Results ===\")\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "for label, prob in sorted_results:\n",
    "    print(f\"{label}: {prob:.4f} ({prob*100:.2f}%)\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 5))\n",
    "labels = [item[0] for item in sorted_results]\n",
    "probs = [item[1] for item in sorted_results]\n",
    "plt.barh(labels, probs)\n",
    "plt.xlabel('Probability')\n",
    "plt.title('CLIP Zero-Shot Classification Results')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-uITTN6zCrM"
   },
   "source": [
    "### üí° What Just Happened?\n",
    "\n",
    "1. We gave CLIP an image and several text descriptions\n",
    "2. CLIP computed **embeddings** (vector representations) for both the image and each text description\n",
    "3. It measured the **similarity** between the image embedding and each text embedding\n",
    "4. Higher similarity = higher probability that the text describes the image\n",
    "\n",
    "**The Magic**: CLIP was never explicitly trained on these specific categories! It learned general image-text relationships from 400M examples.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvBh-QnhzCrM"
   },
   "source": [
    "### ‚úÖ Knowledge Check 2: Zero-Shot Learning\n",
    "\n",
    "**Question 1**: What does \"zero-shot\" mean in the context of CLIP? Why is this capability valuable?\n",
    "\n",
    "**Your Answer**:\n",
    "Zero-shot in CLIP means it can classify or retrieve using new labels without training on labeled examples for that specific task. You provide candidate labels as text prompts, CLIP embeds both the image and the prompts, and it selects the closest match in the shared embedding space. This is valuable because it reduces the need for task-specific labeling and lets you adapt quickly to new categories (open-vocabulary classification).\n",
    "\n",
    "**Question 2**: Look at the classification results above. Try modifying the `categories` list to test different classifications. What happens if you make the categories more specific (e.g., \"a golden retriever\" vs \"a photo of a dog\")?\n",
    "\n",
    "**Your Observations**:\n",
    "Making prompts more specific often shifts the ranking and can improve accuracy because it reduces ambiguity and better matches CLIP‚Äôs training distribution. Prompts like ‚Äúa photo of a golden retriever‚Äù or adding context (photo vs. painting, indoor vs. outdoor) frequently increase the similarity for the correct concept compared to generic prompts like ‚Äúdog.‚Äù\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtP_DC36zCrM"
   },
   "source": [
    "### Experiment 2: Image-Text Retrieval\n",
    "\n",
    "Now let's use CLIP for **image search** - finding images that match a text query. This is similar to Google Images or Pinterest visual search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dIx4GbmZzCrM"
   },
   "outputs": [],
   "source": [
    "# Load multiple images from COCO\n",
    "num_images = 12\n",
    "print(f\"Loading {num_images} images from COCO...\")\n",
    "\n",
    "images = []\n",
    "captions = []\n",
    "\n",
    "for i in range(min(num_images, len(coco_dataset))):\n",
    "    # Download image from URL\n",
    "    image_url = coco_dataset[i]['url']\n",
    "    try:\n",
    "        response = requests.get(image_url, timeout=10)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        images.append(img)\n",
    "\n",
    "        # Get caption\n",
    "        sentences = coco_dataset[i]['sentences']\n",
    "        if isinstance(sentences, list) and len(sentences) > 0:\n",
    "            if isinstance(sentences[0], dict):\n",
    "                captions.append(sentences[0]['raw'])\n",
    "            else:\n",
    "                captions.append(sentences[0])\n",
    "        else:\n",
    "            captions.append(\"No caption available\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load image {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"‚úì Loaded {len(images)} images\")\n",
    "\n",
    "# Display the images\n",
    "show_images(images, titles=[f\"Image {i}\" for i in range(len(images))], cols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "922ZS9BAzCrM"
   },
   "outputs": [],
   "source": [
    "# Image search function\n",
    "def search_images_clip(images, text_query, top_k=3):\n",
    "    \"\"\"\n",
    "    Search for images that match a text query using CLIP.\n",
    "\n",
    "    Args:\n",
    "        images: List of PIL Images\n",
    "        text_query: Text description to search for\n",
    "        top_k: Number of top results to return\n",
    "\n",
    "    Returns:\n",
    "        List of (index, score) tuples for top matches\n",
    "    \"\"\"\n",
    "    # Process all images and the text query\n",
    "    inputs = clip_processor(\n",
    "        text=[text_query],\n",
    "        images=images,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "\n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model(**inputs)\n",
    "\n",
    "    # Calculate similarity scores\n",
    "    logits_per_text = outputs.logits_per_text  # Shape: [1, num_images]\n",
    "    scores = logits_per_text[0].cpu().numpy()\n",
    "\n",
    "    # Get top-k indices\n",
    "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    top_scores = scores[top_indices]\n",
    "\n",
    "    return list(zip(top_indices, top_scores))\n",
    "\n",
    "# Try different search queries\n",
    "queries = [\n",
    "    \"people playing sports\",\n",
    "    \"animals in nature\",\n",
    "    \"food on a table\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\n=== Searching for: '{query}' ===\")\n",
    "    results = search_images_clip(images, query, top_k=3)\n",
    "\n",
    "    # Display results\n",
    "    top_images = [images[idx] for idx, _ in results]\n",
    "    top_titles = [f\"Rank {i+1}\\nScore: {score:.3f}\\nActual: {captions[idx][:50]}...\"\n",
    "                  for i, (idx, score) in enumerate(results)]\n",
    "\n",
    "    show_images(top_images, titles=top_titles, cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fhzq8P76zCrN"
   },
   "source": [
    "### ü§î Reflection Question 2: Search Quality\n",
    "\n",
    "Look at the search results for different queries.\n",
    "\n",
    "**Question**:\n",
    "1. How well did CLIP perform on these searches? Were the top results relevant?\n",
    "2. Try creating your own search queries in the code above. What types of queries work well vs. poorly?\n",
    "3. Why might some queries be harder for CLIP than others? Think about:\n",
    "   - Abstract concepts vs. concrete objects\n",
    "   - Relationships between objects\n",
    "   - Fine-grained details\n",
    "\n",
    "**Your Analysis**:\n",
    "CLIP search is usually strong for concrete, visually distinctive objects (‚Äúdog,‚Äù ‚Äúbus,‚Äù ‚Äúpizza‚Äù) and common scenes because these concepts are well represented in training data. The top results are often relevant, though you may see near-misses when multiple objects co-occur or when the query is ambiguous.\n",
    "\n",
    "Queries that work well: concrete nouns, common activities (‚Äúa person riding a bicycle‚Äù), and simple attributes (‚Äúa red car‚Äù). Queries that work poorly: abstract concepts (‚Äúfreedom‚Äù), subtle relationships (‚Äúthe person holding the smallest object‚Äù), and fine-grained distinctions (specific brands, exact counts, small text).\n",
    "\n",
    "Some queries are harder because CLIP embeddings capture broad semantic similarity more than precise spatial reasoning. Abstract concepts are not directly observable, relationships require structured reasoning about multiple entities, and fine-grained details may be too small or underrepresented to be reliably encoded in the joint embedding space.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3JkL1VUzCrN"
   },
   "source": [
    "### Experiment 3: Understanding Embeddings\n",
    "\n",
    "Let's peek \"under the hood\" and visualize what the embedding space looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_6paTEczCrN"
   },
   "outputs": [],
   "source": [
    "# Extract embeddings for images and text\n",
    "def get_clip_embeddings(images, texts):\n",
    "    \"\"\"\n",
    "    Get CLIP embeddings for images and texts.\n",
    "\n",
    "    Returns:\n",
    "        image_embeddings: numpy array of shape [num_images, embedding_dim]\n",
    "        text_embeddings: numpy array of shape [num_texts, embedding_dim]\n",
    "    \"\"\"\n",
    "    # Process images\n",
    "    image_inputs = clip_processor(images=images, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.get_image_features(**image_inputs)\n",
    "\n",
    "    # Process text\n",
    "    text_inputs = clip_processor(text=texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.get_text_features(**text_inputs)\n",
    "\n",
    "    # Normalize embeddings (CLIP uses cosine similarity)\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    return image_features.cpu().numpy(), text_features.cpu().numpy()\n",
    "\n",
    "# Get embeddings\n",
    "sample_texts = [\n",
    "    \"a photo of a cat\",\n",
    "    \"a photo of a dog\",\n",
    "    \"people playing sports\",\n",
    "    \"delicious food\"\n",
    "]\n",
    "\n",
    "image_embeds, text_embeds = get_clip_embeddings(images[:8], sample_texts)\n",
    "\n",
    "print(f\"Image embeddings shape: {image_embeds.shape}\")\n",
    "print(f\"Text embeddings shape: {text_embeds.shape}\")\n",
    "print(f\"\\nEach item is represented by a {image_embeds.shape[1]}-dimensional vector!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aFhQ9x5jzCrN"
   },
   "outputs": [],
   "source": [
    "# Visualize similarity matrix\n",
    "similarity_matrix = image_embeds @ text_embeds.T  # Matrix multiplication\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(similarity_matrix, cmap='viridis', aspect='auto')\n",
    "plt.colorbar(label='Cosine Similarity')\n",
    "plt.xlabel('Text Descriptions')\n",
    "plt.ylabel('Images')\n",
    "plt.xticks(range(len(sample_texts)), sample_texts, rotation=45, ha='right')\n",
    "plt.yticks(range(len(images[:8])), [f\"Image {i}\" for i in range(8)])\n",
    "plt.title('Image-Text Similarity Matrix (CLIP Embeddings)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"- Brighter colors = higher similarity between image and text\")\n",
    "print(\"- Each cell shows how well an image matches a text description\")\n",
    "print(\"- This is the core of how CLIP performs zero-shot classification!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtoZn-4tzCrN"
   },
   "source": [
    "### ‚úÖ Knowledge Check 3: Embeddings\n",
    "\n",
    "**Question 1**: What is an embedding, and why do we need them? (Hint: Think about how computers represent things)\n",
    "\n",
    "**Your Answer**:\n",
    "An embedding is a learned numeric vector that represents an input (image or text) so that semantic similarity corresponds to geometric similarity. We use embeddings because computers operate on numbers; embeddings turn complex inputs into fixed-length vectors that can be compared efficiently (for retrieval, clustering, and classification).\n",
    "\n",
    "**Question 2**: Look at the similarity matrix above. What does a high similarity score (bright color) tell you about the relationship between an image and a text description?\n",
    "\n",
    "**Your Answer**:\n",
    "A higher value indicates the model thinks the image and that text description are more semantically aligned in the joint embedding space (i.e., the text better matches what‚Äôs in the image). A lower value suggests a weaker match or that other descriptions fit the image better.\n",
    "\n",
    "**Question 3**: CLIP uses **cosine similarity** (measuring the angle between vectors) rather than Euclidean distance. Why might angle be a better measure than distance for semantic similarity?\n",
    "\n",
    "**Your Answer**:\n",
    "Cosine similarity compares direction rather than magnitude, which is helpful because embedding length can vary for reasons unrelated to meaning (scale, normalization, etc.). Direction tends to encode the ‚Äúconcept,‚Äù so angle-based similarity is often more stable for semantic matching than raw distance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIVa_i2HzCrN"
   },
   "source": [
    "### üéØ PATH A CHECKPOINT\n",
    "\n",
    "**Congratulations!** You've completed the core CLIP experiments.\n",
    "\n",
    "**What you've learned so far:**\n",
    "- ‚úÖ How CLIP creates a shared embedding space for images and text\n",
    "- ‚úÖ Zero-shot classification without task-specific training\n",
    "- ‚úÖ Image search using text queries\n",
    "- ‚úÖ How embeddings and similarity scores work\n",
    "\n",
    "**Continue to Section 3** for fine-tuning and advanced topics!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAVV7RyhzCrN"
   },
   "source": [
    "## ‚ö° PATH B: More Resources (BLIP/BLIP-2)\n",
    "\n",
    "### Introduction to BLIP\n",
    "\n",
    "**BLIP (Bootstrapping Language-Image Pre-training)** goes beyond CLIP by adding **generation capabilities**.\n",
    "\n",
    "**Key Features:**\n",
    "- **Understands** images (like CLIP)\n",
    "- **Generates** captions and answers questions (unlike CLIP)\n",
    "- **Bootstraps** training data by generating and filtering its own captions\n",
    "\n",
    "**Architecture:**\n",
    "- Vision Encoder (similar to CLIP)\n",
    "- Q-Former: A \"bridge\" that connects vision to language\n",
    "- Language Model: Generates text\n",
    "\n",
    "**Applications:**\n",
    "1. Image captioning\n",
    "2. Visual Question Answering (VQA)\n",
    "3. Image-text retrieval\n",
    "\n",
    "‚ö†Ô∏è **Note**: BLIP requires more computational resources than CLIP. Make sure you have a GPU with at least 8GB VRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHP0vimezCrN"
   },
   "outputs": [],
   "source": [
    "# PATH B: Load BLIP model for image captioning\n",
    "print(\"Loading BLIP model... (this may take 2-3 minutes)\")\n",
    "\n",
    "# Check if we have enough GPU memory\n",
    "if device == \"cpu\":\n",
    "    print(\"‚ö†Ô∏è Warning: Running BLIP on CPU will be VERY slow.\")\n",
    "    print(\"Consider using Path A (CLIP) instead, or use a GPU environment.\")\n",
    "    # Optionally, uncomment the line below to prevent loading on CPU\n",
    "    # raise Exception(\"BLIP requires GPU. Please use Path A or switch to a GPU environment.\")\n",
    "\n",
    "# Load model for captioning\n",
    "blip_caption_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_caption_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "\n",
    "# Load model for VQA\n",
    "blip_vqa_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "blip_vqa_model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\").to(device)\n",
    "\n",
    "print(\"‚úì BLIP models loaded successfully!\")\n",
    "print(f\"Caption model size: ~990MB\")\n",
    "print(f\"VQA model size: ~990MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b67BW0S_zCrN"
   },
   "source": [
    "### üìö Understanding the Code\n",
    "\n",
    "- **Two separate models**: One for captioning, one for VQA (different tasks, different fine-tuning)\n",
    "- **BlipForConditionalGeneration**: A model that can generate text (captions) from images\n",
    "- **BlipForQuestionAnswering**: Specialized for answering questions about images\n",
    "- These are **task-specific** models, unlike CLIP which is more general-purpose\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM-mhGiXzCrN"
   },
   "outputs": [],
   "source": [
    "# Load sample images (same as Path A)\n",
    "print(\"Loading sample images from COCO dataset...\")\n",
    "coco_dataset = load_dataset(\"yerevann/coco-karpathy\", split=\"validation[:20]\")\n",
    "\n",
    "images_pathb = []\n",
    "captions_pathb = []\n",
    "\n",
    "for i in range(12):\n",
    "    # Download image from URL\n",
    "    image_url = coco_dataset[i]['url']\n",
    "    try:\n",
    "        response = requests.get(image_url, timeout=10)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        images_pathb.append(img)\n",
    "\n",
    "        # Get caption\n",
    "        sentences = coco_dataset[i]['sentences']\n",
    "        if isinstance(sentences, list) and len(sentences) > 0:\n",
    "            if isinstance(sentences[0], dict):\n",
    "                captions_pathb.append(sentences[0]['raw'])\n",
    "            else:\n",
    "                captions_pathb.append(sentences[0])\n",
    "        else:\n",
    "            captions_pathb.append(\"No caption available\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load image {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"‚úì Loaded {len(images_pathb)} images\")\n",
    "show_images(images_pathb[:6], titles=[f\"Image {i}\" for i in range(6)], cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGYLj-3KzCrN"
   },
   "source": [
    "### Experiment 1: Image Captioning\n",
    "\n",
    "Let's generate natural language descriptions of images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3MSsKV1zCrN"
   },
   "outputs": [],
   "source": [
    "# Generate captions\n",
    "def generate_caption_blip(image):\n",
    "    \"\"\"\n",
    "    Generate a caption for an image using BLIP.\n",
    "\n",
    "    Args:\n",
    "        image: PIL Image\n",
    "\n",
    "    Returns:\n",
    "        Generated caption (string)\n",
    "    \"\"\"\n",
    "    # Process image\n",
    "    inputs = blip_caption_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate caption\n",
    "    with torch.no_grad():\n",
    "        output = blip_caption_model.generate(**inputs, max_length=50)\n",
    "\n",
    "    # Decode the output\n",
    "    caption = blip_caption_processor.decode(output[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "# Test on several images\n",
    "print(\"\\n=== Generating Captions ===\")\n",
    "for i in range(6):\n",
    "    image = images_pathb[i]\n",
    "    generated_caption = generate_caption_blip(image)\n",
    "    actual_caption = captions_pathb[i]\n",
    "\n",
    "    print(f\"\\nImage {i+1}:\")\n",
    "    print(f\"Generated: {generated_caption}\")\n",
    "    print(f\"Actual:    {actual_caption}\")\n",
    "\n",
    "# Visualize\n",
    "sample_idx = 0\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(images_pathb[sample_idx])\n",
    "plt.axis('off')\n",
    "generated = generate_caption_blip(images_pathb[sample_idx])\n",
    "plt.title(f\"Generated Caption:\\n{generated}\", fontsize=12, wrap=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGCnLvqjzCrN"
   },
   "source": [
    "### üí° What Just Happened?\n",
    "\n",
    "1. BLIP's vision encoder converted the image into embeddings\n",
    "2. The Q-Former \"queries\" these embeddings to extract relevant information\n",
    "3. The language model generates text, token by token, based on the visual information\n",
    "4. This is **generative** - BLIP is creating new text, not just classifying!\n",
    "\n",
    "**Key Difference from CLIP**: CLIP can only match/score existing text. BLIP can generate new descriptions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZM9Hq3AzCrN"
   },
   "source": [
    "### ‚úÖ Knowledge Check 2 (Path B): Generation vs. Classification\n",
    "\n",
    "**Question 1**: What is the fundamental difference between CLIP's zero-shot classification and BLIP's caption generation? Think about their outputs.\n",
    "\n",
    "**Your Answer**:\n",
    "CLIP is primarily discriminative/retrieval-based: it outputs similarity scores between an image and a set of text labels (often treated as classification probabilities). BLIP is generative: it conditions on the image and produces natural-language text (a caption) as a sequence of tokens.\n",
    "\n",
    "**Question 2**: Look at the generated captions vs. actual captions. How well did BLIP do? What kinds of details did it capture? What did it miss?\n",
    "\n",
    "**Your Analysis**:\n",
    "The generated captions typically capture the main objects and actions (the most salient content), but they may miss fine details such as exact counts, small text, or subtle attributes (brand/model). When the scene is ambiguous, captions can become generic. Occasionally, BLIP may add plausible details that are not actually present (hallucination).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grCHsaElzCrN"
   },
   "source": [
    "### Experiment 2: Visual Question Answering (VQA)\n",
    "\n",
    "Now let's ask questions about images! This is more interactive than captioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8RUkfGPKzCrN"
   },
   "outputs": [],
   "source": [
    "# VQA function\n",
    "def answer_question_blip(image, question):\n",
    "    \"\"\"\n",
    "    Answer a question about an image using BLIP.\n",
    "\n",
    "    Args:\n",
    "        image: PIL Image\n",
    "        question: Question string\n",
    "\n",
    "    Returns:\n",
    "        Answer string\n",
    "    \"\"\"\n",
    "    # Process image and question\n",
    "    inputs = blip_vqa_processor(images=image, text=question, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate answer\n",
    "    with torch.no_grad():\n",
    "        output = blip_vqa_model.generate(**inputs, max_length=20)\n",
    "\n",
    "    # Decode the answer\n",
    "    answer = blip_vqa_processor.decode(output[0], skip_special_tokens=True)\n",
    "    return answer\n",
    "\n",
    "# Test with different questions\n",
    "test_image_idx = 2\n",
    "test_image = images_pathb[test_image_idx]\n",
    "\n",
    "questions = [\n",
    "    \"What is in the image?\",\n",
    "    \"What color is the main object?\",\n",
    "    \"Is this indoors or outdoors?\",\n",
    "    \"How many people are in the image?\",\n",
    "    \"What is the person doing?\"\n",
    "]\n",
    "\n",
    "# Display image\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(test_image)\n",
    "plt.axis('off')\n",
    "plt.title(f\"Test Image {test_image_idx+1}\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nActual caption: {captions_pathb[test_image_idx]}\")\n",
    "print(\"\\n=== Visual Question Answering ===\")\n",
    "for question in questions:\n",
    "    answer = answer_question_blip(test_image, question)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {answer}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IctYRxcGzCrN"
   },
   "source": [
    "### ü§î Reflection Question 2 (Path B): VQA Capabilities\n",
    "\n",
    "**Question**:\n",
    "1. Look at the Q&A pairs above. Which types of questions did BLIP answer well? Which were challenging?\n",
    "2. Try adding your own questions in the code above. What types of reasoning does VQA require?\n",
    "   - Simple object detection (\"What is in the image?\")\n",
    "   - Counting (\"How many...?\")\n",
    "   - Spatial relationships (\"Where is...?\")\n",
    "   - Contextual understanding (\"Why is...?\")\n",
    "3. What are the limitations you notice? Think about hallucinations mentioned in the lecture.\n",
    "\n",
    "**Your Analysis**:\n",
    "VQA tends to work best on questions that are close to object detection/recognition (‚ÄúWhat is in the image?‚Äù, ‚ÄúWhat animal is shown?‚Äù) and some straightforward attribute questions (colors, common actions), because these align with strong visual cues.\n",
    "\n",
    "It is usually challenged by counting, reading small text, and precise spatial reasoning (‚ÄúHow many‚Ä¶?‚Äù, ‚ÄúWhat is written‚Ä¶?‚Äù, ‚ÄúIs the cup to the left of the plate?‚Äù), and especially by ‚Äúwhy‚Äù questions that require external knowledge or multi-step inference.\n",
    "\n",
    "The key limitations are hallucination and overconfidence: the model may answer fluently even when the image does not support the claim. Performance also degrades on uncommon objects, occlusions, low-resolution details, and domain-shifted images (medical, industrial, etc.).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j82xrONYzCrN"
   },
   "source": [
    "### Experiment 3: Comparing Captioning Approaches\n",
    "\n",
    "BLIP can generate captions in different ways. Let's explore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ThIgQVWEzCrN"
   },
   "outputs": [],
   "source": [
    "# Conditional captioning with BLIP\n",
    "def generate_conditional_caption(image, prompt):\n",
    "    \"\"\"\n",
    "    Generate a caption with a text prompt/prefix.\n",
    "\n",
    "    Args:\n",
    "        image: PIL Image\n",
    "        prompt: Text prompt to condition the generation\n",
    "\n",
    "    Returns:\n",
    "        Generated caption\n",
    "    \"\"\"\n",
    "    # Process with text prompt\n",
    "    inputs = blip_caption_processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        output = blip_caption_model.generate(**inputs, max_length=50)\n",
    "\n",
    "    caption = blip_caption_processor.decode(output[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "# Test with different prompts\n",
    "test_img = images_pathb[0]\n",
    "\n",
    "prompts = [\n",
    "    \"\",  # Unconditional\n",
    "    \"a photo of\",\n",
    "    \"this image shows\",\n",
    "    \"in this scene,\"\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(test_img)\n",
    "plt.axis('off')\n",
    "plt.title(\"Test Image\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Conditional Captioning ===\")\n",
    "for prompt in prompts:\n",
    "    caption = generate_conditional_caption(test_img, prompt)\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"Caption: {caption}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vcTfW_TzCrT"
   },
   "source": [
    "### ‚úÖ Knowledge Check 3 (Path B): Conditional Generation\n",
    "\n",
    "**Question**: How does the text prompt influence the generated caption? Why might conditional generation be useful in real applications?\n",
    "\n",
    "**Your Answer**:\n",
    "In conditional generation, the prompt acts like an instruction that steers what the model should say (length, style, focus, or specific attributes to mention). For example, asking ‚ÄúDescribe the colors and clothing‚Äù will bias the output toward those aspects, while ‚ÄúDescribe the scene in one sentence‚Äù will shorten and simplify the caption.\n",
    "\n",
    "Conditional generation is useful because it enables controllable outputs for different business needs (product listings, accessibility descriptions, moderation summaries, educational explanations) without retraining a new model for each format.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZV5ounCzCrT"
   },
   "source": [
    "### üéØ PATH B CHECKPOINT\n",
    "\n",
    "**Congratulations!** You've completed the core BLIP experiments.\n",
    "\n",
    "**What you've learned so far:**\n",
    "- ‚úÖ How BLIP generates captions from images\n",
    "- ‚úÖ Visual Question Answering (VQA) capabilities\n",
    "- ‚úÖ Conditional vs. unconditional generation\n",
    "- ‚úÖ Comparing understanding vs. generation models\n",
    "\n",
    "**Continue to Section 3** for fine-tuning and advanced topics!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quxGPslmzCrT"
   },
   "source": [
    "## Section 3: Introduction to Fine-Tuning\n",
    "\n",
    "So far, we've used **pre-trained** models. But what if we want to adapt them to our specific use case?\n",
    "\n",
    "### What is Fine-Tuning?\n",
    "\n",
    "**Fine-tuning** means taking a pre-trained model and training it further on a specific dataset or task.\n",
    "\n",
    "**Why Fine-Tune?**\n",
    "- Improve performance on your specific domain (e.g., medical images, product photos)\n",
    "- Adapt to your specific task (e.g., detecting specific objects)\n",
    "- Handle specialized vocabulary or visual concepts\n",
    "\n",
    "**The Trade-off:**\n",
    "- ‚úÖ Better performance on your task\n",
    "- ‚ùå Requires labeled data\n",
    "- ‚ùå Requires computational resources\n",
    "- ‚ùå Risk of overfitting (losing general knowledge)\n",
    "\n",
    "### Fine-Tuning Strategies\n",
    "\n",
    "1. **Full Fine-Tuning**: Update all model parameters (expensive, powerful)\n",
    "2. **Partial Fine-Tuning**: Only update some layers (more efficient)\n",
    "3. **Adapter Layers**: Add small trainable modules (very efficient)\n",
    "4. **Prompt Tuning**: Learn optimal text prompts (minimal parameters)\n",
    "\n",
    "In this lab, we'll do a **simple demonstration** of fine-tuning concepts rather than full training (which would take hours).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBlbjAzczCrT"
   },
   "source": [
    "### ü§î Reflection Question 3: Fine-Tuning Considerations\n",
    "\n",
    "Before we proceed, think about this scenario:\n",
    "\n",
    "**Scenario**: A hospital wants to use a VLM to help radiologists by automatically generating descriptions of X-ray images.\n",
    "\n",
    "**Questions**:\n",
    "1. Would a pre-trained model like BLIP (trained on general internet images) work well out-of-the-box? Why or why not?\n",
    "2. What challenges would you face in fine-tuning a VLM for this task?\n",
    "3. What kind of data would you need? How much?\n",
    "4. What are the ethical considerations? (Think back to the lecture on VLM limitations)\n",
    "\n",
    "**Your Answer**:\n",
    "1) A general pre-trained model would likely perform poorly out-of-the-box because medical X-rays differ substantially from internet photos (domain shift) and the required language (radiology terminology, clinically relevant findings) is specialized.\n",
    "2) Fine-tuning challenges include limited labeled data, strict privacy/compliance requirements (HIPAA), high stakes for errors, class imbalance (rare findings), and the need for careful evaluation and calibration to avoid hallucinated findings.\n",
    "3) You would need paired X-ray images and high-quality ground-truth reports/labels (radiologist-verified). Practically, you often need at least many thousands of studies (and ideally tens of thousands+) depending on scope and label granularity, plus a clean validation/test set.\n",
    "4) Ethical considerations: patient privacy and consent, bias across populations and equipment, risk of automation bias (clinicians over-trusting outputs), explainability/auditability, and clear human oversight‚Äîoutputs should be assistive and reviewed by qualified clinicians, not treated as autonomous diagnoses.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlDjq5xHzCrT"
   },
   "source": [
    "### Demonstration: Understanding Model Parameters\n",
    "\n",
    "Let's explore what \"training\" actually means at a technical level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_J0B5tzszCrT"
   },
   "outputs": [],
   "source": [
    "# Explore model architecture (works for both paths)\n",
    "# Path A users: examine clip_model\n",
    "# Path B users: examine blip_caption_model\n",
    "\n",
    "# Let's use the model you loaded earlier\n",
    "try:\n",
    "    model_to_examine = clip_model\n",
    "    model_name_str = \"CLIP\"\n",
    "except:\n",
    "    model_to_examine = blip_caption_model\n",
    "    model_name_str = \"BLIP\"\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model_to_examine.parameters())\n",
    "trainable_params = sum(p.numel() for p in model_to_examine.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"=== {model_name_str} Model Statistics ===\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\nParameter size: ~{total_params * 4 / 1e9:.2f} GB (32-bit floats)\")\n",
    "\n",
    "# Look at model structure\n",
    "print(f\"\\n=== Model Architecture (first few layers) ===\")\n",
    "for name, module in list(model_to_examine.named_modules())[:10]:\n",
    "    print(f\"{name}: {module.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkVu5GynzCrT"
   },
   "source": [
    "### üí° Understanding Parameters\n",
    "\n",
    "- **Parameters**: The \"knobs\" that the model adjusts during training (weights and biases)\n",
    "- **Trainable parameters**: Parameters that will be updated during fine-tuning\n",
    "- More parameters = more capacity to learn, but also more data and compute needed\n",
    "\n",
    "**Example**: CLIP has ~150M parameters. Training all of them requires:\n",
    "- Lots of data (millions of examples)\n",
    "- Lots of compute (GPUs for days/weeks)\n",
    "- Lots of memory (several GB)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRiDtEu6zCrU"
   },
   "source": [
    "### Concept Demo: Freezing Layers\n",
    "\n",
    "A common technique is to \"freeze\" most of the model and only train the last few layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SO_BAM5GzCrU"
   },
   "outputs": [],
   "source": [
    "# Demonstrate freezing parameters\n",
    "import copy\n",
    "\n",
    "# Create a copy to demonstrate (we won't actually train it)\n",
    "demo_model = copy.deepcopy(model_to_examine)\n",
    "\n",
    "# Freeze all parameters\n",
    "for param in demo_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"=== After Freezing All Parameters ===\")\n",
    "trainable = sum(p.numel() for p in demo_model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable:,}\")\n",
    "print(\"\\nNow let's unfreeze the last layer...\")\n",
    "\n",
    "# Unfreeze last few layers (this is model-specific)\n",
    "# For demonstration, let's unfreeze projection layers\n",
    "if model_name_str == \"CLIP\":\n",
    "    # Unfreeze projection layers\n",
    "    for param in demo_model.visual_projection.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in demo_model.text_projection.parameters():\n",
    "        param.requires_grad = True\n",
    "else:\n",
    "    # For BLIP, unfreeze text decoder\n",
    "    for param in list(demo_model.parameters())[-10:]:\n",
    "        param.requires_grad = True\n",
    "\n",
    "trainable_after = sum(p.numel() for p in demo_model.parameters() if p.requires_grad)\n",
    "print(f\"\\n=== After Unfreezing Last Layers ===\")\n",
    "print(f\"Trainable parameters: {trainable_after:,}\")\n",
    "print(f\"Percentage of total: {trainable_after/total_params*100:.2f}%\")\n",
    "print(f\"\\nüí° By freezing most layers, we only need to optimize {trainable_after:,} parameters!\")\n",
    "print(\"This is MUCH faster and requires less data.\")\n",
    "\n",
    "# Clean up\n",
    "del demo_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lEIxP8VzCrU"
   },
   "source": [
    "### ‚úÖ Knowledge Check 4: Fine-Tuning Strategy\n",
    "\n",
    "**Question 1**: Why might we want to freeze most of a pre-trained model and only train the last few layers?\n",
    "\n",
    "**Your Answer**:\n",
    "Freezing most layers keeps the broad visual/language knowledge from pre-training, reduces compute, and lowers overfitting risk when task data is limited. Training only the last layers adapts the model to the specific task while keeping core representations stable.\n",
    "\n",
    "**Question 2**: What's the risk of fine-tuning too aggressively (training all layers on a small dataset)?\n",
    "\n",
    "**Your Answer**:\n",
    "If you fine-tune all layers on a small dataset, the model can overfit and suffer catastrophic forgetting‚Äîlosing general capabilities and becoming biased toward the small training set‚Äîreducing real-world generalization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WE6fZh7ezCrU"
   },
   "source": [
    "### Demonstration: Training Loop Concepts\n",
    "\n",
    "Let's understand what happens during training, even if we don't run a full training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSxFX_HDzCrU"
   },
   "outputs": [],
   "source": [
    "# Pseudo-code for fine-tuning (for educational purposes)\n",
    "# DO NOT RUN - this is just to illustrate the concept\n",
    "\n",
    "print(\"=== Fine-Tuning Process (Conceptual) ===\")\n",
    "print(\"\"\"\n",
    "# 1. Prepare your data\n",
    "train_dataset = load_your_custom_dataset()\n",
    "train_loader = DataLoader(train_dataset, batch_size=32)\n",
    "\n",
    "# 2. Freeze most of the model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze last layers\n",
    "for param in model.last_layer.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 3. Set up optimizer (adjusts parameters)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# 4. Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(batch['images'], batch['text'])\n",
    "\n",
    "        # Compute loss (how wrong are we?)\n",
    "        loss = compute_loss(outputs, batch['labels'])\n",
    "\n",
    "        # Backward pass (compute gradients)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "# 5. Evaluate on validation set\n",
    "evaluate(model, val_loader)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüí° Key Concepts:\")\n",
    "print(\"- Loss function: Measures how wrong the model's predictions are\")\n",
    "print(\"- Gradients: Tell us how to adjust each parameter to reduce loss\")\n",
    "print(\"- Optimizer: Updates parameters based on gradients\")\n",
    "print(\"- Epoch: One complete pass through the training data\")\n",
    "print(\"- Learning rate: How big of steps to take when updating parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVlJZgK5zCrU"
   },
   "source": [
    "### Quick Example: Loss Calculation\n",
    "\n",
    "Let's see what a loss function looks like for VLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vwc_6qwnzCrU"
   },
   "outputs": [],
   "source": [
    "# Simple demonstration of contrastive loss (used in CLIP)\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Simulate image and text embeddings (normalized)\n",
    "# In reality, these come from the model\n",
    "batch_size = 4\n",
    "embed_dim = 512\n",
    "\n",
    "# Create random embeddings (just for demonstration)\n",
    "image_embeds = torch.randn(batch_size, embed_dim)\n",
    "text_embeds = torch.randn(batch_size, embed_dim)\n",
    "\n",
    "# Normalize\n",
    "image_embeds = F.normalize(image_embeds, dim=-1)\n",
    "text_embeds = F.normalize(text_embeds, dim=-1)\n",
    "\n",
    "# Compute similarity matrix\n",
    "similarity = image_embeds @ text_embeds.T\n",
    "\n",
    "print(\"=== Contrastive Learning Demo ===\")\n",
    "print(f\"\\nSimilarity matrix shape: {similarity.shape}\")\n",
    "print(f\"Similarity matrix (each row = 1 image vs 4 texts):\")\n",
    "print(similarity.numpy())\n",
    "\n",
    "# In training, we want diagonal to be high (correct matches)\n",
    "# and off-diagonal to be low (incorrect matches)\n",
    "print(\"\\nüí° Goal:\")\n",
    "print(\"- Diagonal values (correct image-text pairs) should be HIGH\")\n",
    "print(\"- Off-diagonal values (incorrect pairs) should be LOW\")\n",
    "print(\"\\nThe loss function pushes the model toward this goal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vDTWH3OzCrU"
   },
   "source": [
    "### ü§î Reflection Question 4: Training Data Quality\n",
    "\n",
    "From the lecture, we learned about BLIP's **bootstrapping** approach - it generates its own captions, filters them for quality, and retrains on the improved data.\n",
    "\n",
    "**Questions**:\n",
    "1. Why is data quality so important for fine-tuning VLMs?\n",
    "2. What could go wrong if you fine-tune on low-quality or biased data?\n",
    "3. How might BLIP's bootstrapping help improve data quality?\n",
    "4. What are the risks of a model training on its own outputs?\n",
    "\n",
    "**Your Answer**:\n",
    "1) Data quality matters because the model will learn whatever patterns the data contains‚Äîgood or bad. High-quality, well-aligned image‚Äìtext pairs teach the model correct associations; noisy captions/labels teach incorrect ones.\n",
    "2) Low-quality or biased data can cause systematic errors (hallucinations, spurious correlations), amplify demographic biases, and reduce generalization, leading to unreliable or unfair outputs in deployment.\n",
    "3) Bootstrapping can help by generating candidate captions, filtering them using quality checks (e.g., consistency, confidence, human review), and retraining on the cleaner subset‚Äîeffectively denoising and expanding training data.\n",
    "4) Risks of training on its own outputs include feedback loops that reinforce mistakes, amplify biases, reduce diversity in language, and ‚Äúmodel collapse‚Äù where the system learns increasingly generic or incorrect patterns because the errors become part of the training signal.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euL5oWz9zCrU"
   },
   "source": [
    "## Section 4: Evaluation and Metrics\n",
    "\n",
    "How do we know if our VLM is working well? We need metrics!\n",
    "\n",
    "### Recall from Lecture: VLM Performance Metrics\n",
    "\n",
    "**For Classification/Retrieval:**\n",
    "- **Accuracy**: % of correct predictions\n",
    "- **Recall@K**: Are the correct items in the top K results?\n",
    "\n",
    "**For Generation (Captions):**\n",
    "- **BLEU**: Measures n-gram overlap with reference captions\n",
    "- **METEOR**: Similar to BLEU but accounts for synonyms\n",
    "- **CIDEr**: Designed specifically for image captioning\n",
    "\n",
    "**For VQA:**\n",
    "- **VQA Accuracy**: % of correct answers\n",
    "\n",
    "**Subjective:**\n",
    "- **Human Evaluation**: People rate quality and relevance\n",
    "\n",
    "Let's implement some of these!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVdKcfqTzCrU"
   },
   "source": [
    "### Evaluation Experiment 1: Retrieval Performance\n",
    "\n",
    "Let's evaluate how well our model can retrieve relevant images for text queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yrvkMMmJzCrU"
   },
   "outputs": [],
   "source": [
    "# Recall@K metric\n",
    "def calculate_recall_at_k(retrieved_indices, relevant_indices, k):\n",
    "    \"\"\"\n",
    "    Calculate Recall@K: Are any of the relevant items in the top K results?\n",
    "\n",
    "    Args:\n",
    "        retrieved_indices: List of retrieved item indices (ranked)\n",
    "        relevant_indices: Set of relevant item indices\n",
    "        k: Number of top results to consider\n",
    "\n",
    "    Returns:\n",
    "        Recall@K score (0 to 1)\n",
    "    \"\"\"\n",
    "    top_k = set(retrieved_indices[:k])\n",
    "    relevant = set(relevant_indices)\n",
    "\n",
    "    # How many relevant items did we retrieve?\n",
    "    hits = len(top_k.intersection(relevant))\n",
    "\n",
    "    # Recall = hits / total relevant items\n",
    "    recall = hits / len(relevant) if len(relevant) > 0 else 0\n",
    "    return recall\n",
    "\n",
    "# Example: Evaluate search performance\n",
    "print(\"=== Evaluating Image Retrieval ===\")\n",
    "\n",
    "# For PATH A (CLIP)\n",
    "if 'clip_model' in dir():\n",
    "    # Create test queries with known relevant images\n",
    "    # In a real scenario, you'd have ground truth labels\n",
    "    test_query = \"people playing sports\"\n",
    "\n",
    "    # Search\n",
    "    results = search_images_clip(images[:12], test_query, top_k=5)\n",
    "    retrieved_indices = [idx for idx, _ in results]\n",
    "\n",
    "    # Manually define relevant images (you'd normally have this from dataset labels)\n",
    "    # For demo, let's assume images 2, 5, 7 are relevant (you'd verify this)\n",
    "    relevant_indices = [2, 5, 7]  # This would come from annotations\n",
    "\n",
    "    # Calculate metrics\n",
    "    recall_at_3 = calculate_recall_at_k(retrieved_indices, relevant_indices, k=3)\n",
    "    recall_at_5 = calculate_recall_at_k(retrieved_indices, relevant_indices, k=5)\n",
    "\n",
    "    print(f\"Query: '{test_query}'\")\n",
    "    print(f\"Retrieved indices (top 5): {retrieved_indices}\")\n",
    "    print(f\"Relevant indices: {relevant_indices}\")\n",
    "    print(f\"\\nRecall@3: {recall_at_3:.3f}\")\n",
    "    print(f\"Recall@5: {recall_at_5:.3f}\")\n",
    "    print(\"\\nüí° Interpretation:\")\n",
    "    print(f\"  - Recall@3: {recall_at_3*100:.0f}% of relevant images were in top 3 results\")\n",
    "    print(f\"  - Recall@5: {recall_at_5*100:.0f}% of relevant images were in top 5 results\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è This evaluation is designed for Path A (CLIP)\")\n",
    "    print(\"For Path B, we'll evaluate caption quality instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xe-L-8JyzCrU"
   },
   "source": [
    "### Evaluation Experiment 2: Caption Quality (Path B)\n",
    "\n",
    "For generative models, we need different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Kav4fI6zCrU"
   },
   "outputs": [],
   "source": [
    "# Simple BLEU score implementation\n",
    "# (In practice, use libraries like sacrebleu or nltk)\n",
    "\n",
    "def simple_bleu_score(generated, reference):\n",
    "    \"\"\"\n",
    "    Very simplified BLEU-1 score (unigram overlap).\n",
    "    Real BLEU uses n-grams and penalties.\n",
    "\n",
    "    Args:\n",
    "        generated: Generated caption (string)\n",
    "        reference: Reference caption (string)\n",
    "\n",
    "    Returns:\n",
    "        Score between 0 and 1\n",
    "    \"\"\"\n",
    "    gen_words = set(generated.lower().split())\n",
    "    ref_words = set(reference.lower().split())\n",
    "\n",
    "    if len(gen_words) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    overlap = len(gen_words.intersection(ref_words))\n",
    "    score = overlap / len(gen_words)\n",
    "    return score\n",
    "\n",
    "# Evaluate caption quality (Path B)\n",
    "if 'blip_caption_model' in dir():\n",
    "    print(\"=== Evaluating Caption Quality (BLIP) ===\")\n",
    "\n",
    "    scores = []\n",
    "    for i in range(5):\n",
    "        image = images_pathb[i]\n",
    "        generated = generate_caption_blip(image)\n",
    "        reference = captions_pathb[i]\n",
    "\n",
    "        score = simple_bleu_score(generated, reference)\n",
    "        scores.append(score)\n",
    "\n",
    "        print(f\"\\nImage {i+1}:\")\n",
    "        print(f\"Generated: {generated}\")\n",
    "        print(f\"Reference: {reference}\")\n",
    "        print(f\"BLEU-1 Score: {score:.3f}\")\n",
    "\n",
    "    avg_score = np.mean(scores)\n",
    "    print(f\"\\n=== Average BLEU-1 Score: {avg_score:.3f} ===\")\n",
    "    print(\"\\nüí° Note: This is a simplified metric.\")\n",
    "    print(\"Real BLEU considers multiple n-grams and brevity penalties.\")\n",
    "else:\n",
    "    print(\"=== Caption Evaluation Demo (Conceptual) ===\")\n",
    "    print(\"\\nFor Path A users: CLIP doesn't generate captions.\")\n",
    "    print(\"Caption quality metrics like BLEU are used for generative models.\")\n",
    "    print(\"\\nExample:\")\n",
    "    generated = \"a dog playing with a ball\"\n",
    "    reference = \"a golden retriever playing with a red ball in a park\"\n",
    "    score = simple_bleu_score(generated, reference)\n",
    "    print(f\"Generated: {generated}\")\n",
    "    print(f\"Reference: {reference}\")\n",
    "    print(f\"BLEU-1 Score: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ggmvhFdZzCrV"
   },
   "source": [
    "### ‚úÖ Knowledge Check 5: Metrics and Evaluation\n",
    "\n",
    "**Question 1**: Why do we use different metrics for retrieval (Recall@K) vs. generation (BLEU)?\n",
    "\n",
    "**Your Answer**:\n",
    "Retrieval is a ranking problem, so metrics like Recall@K evaluate whether relevant items appear near the top of the returned list. Generation has many valid wordings, so overlap-based text metrics like BLEU estimate how close the generated text is to reference text.\n",
    "\n",
    "**Question 2**: What are the limitations of automatic metrics like BLEU? Can a caption have a low BLEU score but still be good? Give an example.\n",
    "\n",
    "**Your Answer**:\n",
    "BLEU (and similar automatic metrics) can penalize correct paraphrases and synonyms because they reward exact n-gram overlap. Example: reference ‚Äúa man riding a bike‚Äù vs. prediction ‚Äúa person on a bicycle‚Äù‚Äîsemantically correct, but BLEU may be low due to different wording.\n",
    "\n",
    "**Question 3**: From the lecture, we learned about human evaluation. Why is human evaluation still important despite having automatic metrics?\n",
    "\n",
    "**Your Answer**:\n",
    "Human evaluation is important because it captures usefulness, factual correctness, safety, and nuance (e.g., hallucinations, bias) that automatic metrics often miss or correlate with weakly, especially in domain-specific contexts.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1j-Xi1NzCrV"
   },
   "source": [
    "### ü§î Reflection Question 5: Evaluation in Practice\n",
    "\n",
    "**Scenario**: You're deploying a VLM for product search on an e-commerce website.\n",
    "\n",
    "**Questions**:\n",
    "1. What metrics would you track in production? (Think beyond just accuracy)\n",
    "2. How would you detect if the model's performance is degrading over time?\n",
    "3. What business metrics would you care about? (Hint: Not just technical metrics)\n",
    "4. How would you handle cases where the model makes mistakes?\n",
    "\n",
    "**Your Answer**:\n",
    "1) Technical/product metrics: Recall@K or NDCG for retrieval quality, click-through rate on results, add-to-cart/conversion after search, query latency, error rate, and coverage (% queries returning good results).\n",
    "2) Detect degradation via continuous monitoring on a held-out ‚Äúgolden‚Äù set, sampling and manually reviewing real queries, tracking distribution drift in embeddings/queries, and running periodic A/B tests against a baseline.\n",
    "3) Business metrics: conversion rate, revenue per search session, bounce rate, time-to-find-product, return rate (if wrong products are chosen), and customer satisfaction/NPS.\n",
    "4) Handle mistakes with fallbacks (keyword search + filters), user feedback (‚Äúnot relevant‚Äù buttons), safe abstention (‚ÄúI‚Äôm not sure‚Äù/ask clarifying questions), and human review loops for high-impact categories.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovl788-hzCrV"
   },
   "source": [
    "## Section 5: Real-World Applications and Trade-offs\n",
    "\n",
    "Now let's bring it all together by exploring real-world applications and analyzing trade-offs.\n",
    "\n",
    "### Recall from Lecture: VLM Applications\n",
    "\n",
    "- üõçÔ∏è **Retail & E-commerce**: Visual product search, auto-tagging\n",
    "- ‚ôø **Accessibility**: Image descriptions for visually impaired users\n",
    "- üè≠ **Manufacturing**: Quality control, defect detection\n",
    "- üè• **Healthcare**: Medical image analysis, report generation\n",
    "- üéì **Education**: Interactive learning, automatic assessment\n",
    "- üõ°Ô∏è **Content Moderation**: Detecting inappropriate content\n",
    "- üí¨ **Customer Support**: Visual chatbots, issue diagnosis\n",
    "\n",
    "Let's implement a mini real-world application!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEHpVjKWzCrV"
   },
   "source": [
    "### Application Demo: Product Search System\n",
    "\n",
    "Let's build a simple visual product search system!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_UMumg1zCrV"
   },
   "outputs": [],
   "source": [
    "# Simulate a product catalog\n",
    "print(\"=== Visual Product Search Demo ===\")\n",
    "print(\"\\nSimulating a product catalog with images from COCO...\")\n",
    "\n",
    "# Use our loaded images as \"products\"\n",
    "product_images = images[:10] if 'images' in dir() else images_pathb[:10]\n",
    "product_names = [f\"Product {i+1}\" for i in range(len(product_images))]\n",
    "\n",
    "# Display the \"catalog\"\n",
    "print(f\"\\nCatalog size: {len(product_images)} products\")\n",
    "show_images(product_images, titles=product_names, cols=5)\n",
    "\n",
    "# Interactive search\n",
    "print(\"\\n=== Try These Search Queries ===\")\n",
    "sample_queries = [\n",
    "    \"outdoor sports equipment\",\n",
    "    \"person wearing a hat\",\n",
    "    \"colorful objects\",\n",
    "    \"indoor furniture\"\n",
    "]\n",
    "\n",
    "for query in sample_queries:\n",
    "    print(f\"\\nüìù Query: '{query}'\")\n",
    "\n",
    "    if 'clip_model' in dir():\n",
    "        # Use CLIP for search\n",
    "        results = search_images_clip(product_images, query, top_k=3)\n",
    "        print(\"Top 3 Results:\")\n",
    "        for rank, (idx, score) in enumerate(results, 1):\n",
    "            print(f\"  {rank}. {product_names[idx]} (score: {score:.3f})\")\n",
    "    else:\n",
    "        print(\"  [Search would use CLIP embeddings]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVqAi19azCrV"
   },
   "source": [
    "### ü§î Reflection Question 6: Architecture Selection\n",
    "\n",
    "Recall the architecture comparison from the lecture:\n",
    "\n",
    "| Architecture | Approach | Best For |\n",
    "|-------------|----------|----------|\n",
    "| CLIP | Contrastive | Classification, search |\n",
    "| BLIP | Projector + Gen | VQA, Captioning |\n",
    "| LLaVA | Projector | Multimodal chat |\n",
    "| Flamingo | Adapter | Few-shot learning |\n",
    "| GPT-4V | Integrated | Complex reasoning |\n",
    "\n",
    "**Question**: For each application below, which VLM architecture would you choose? Explain your reasoning considering:\n",
    "- Task requirements\n",
    "- Performance needs\n",
    "- Cost constraints\n",
    "- Data availability\n",
    "\n",
    "**Applications**:\n",
    "1. **Museum exhibit descriptions**: Generate engaging descriptions of artwork for visitors\n",
    "2. **E-commerce search**: Let customers find products using natural language\n",
    "3. **Medical diagnosis assistant**: Help doctors analyze X-rays and generate reports\n",
    "4. **Social media content moderation**: Flag inappropriate images\n",
    "5. **Educational chatbot**: Answer student questions about diagrams and figures\n",
    "\n",
    "**Your Analysis**:\n",
    "1) **Museum**: LLaVA-style multimodal chat (or an integrated model like GPT‚Äë4V) **plus RAG** over the museum‚Äôs catalog. Rationale: users want multi-turn Q&A and storytelling grounded in curated facts; retrieval reduces hallucinations.\n",
    "\n",
    "2) **E-commerce**: CLIP for scalable product search, tagging, and ‚Äúfind similar,‚Äù optionally paired with a lightweight generator for short descriptions. Rationale: retrieval quality and latency matter most at scale; CLIP is efficient and cost-effective.\n",
    "\n",
    "3) **Medical**: A domain-specific, fine-tuned captioning/VQA model (BLIP-style) hosted in a compliant environment, with strict human review. Rationale: domain shift requires specialization; high stakes demand auditability, privacy, and abstention/escalation.\n",
    "\n",
    "4) **Moderation**: CLIP-style contrastive model for fast classification/filtering at very high volume, optionally followed by a multimodal LLM for borderline cases/explanations. Rationale: cost and throughput are critical; a two-stage pipeline balances speed with nuance.\n",
    "\n",
    "5) **Education**: LLaVA or GPT‚Äë4V-style model for interactive tutoring with images, grounded with RAG to course content. Rationale: conversation and reasoning are central; grounding keeps explanations accurate and aligned to the curriculum.\n",
    "\n",
    "1. Museum:\n",
    "\n",
    "2. E-commerce:\n",
    "\n",
    "3. Medical:\n",
    "\n",
    "4. Moderation:\n",
    "\n",
    "5. Education:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cWL-FHbzCrV"
   },
   "source": [
    "### Trade-off Analysis: API vs. Self-Hosted\n",
    "\n",
    "Recall from lecture: **API Services** (GPT-4V, Claude) vs. **Self-Hosted** (CLIP, BLIP, LLaVA)\n",
    "\n",
    "Let's analyze the trade-offs with real numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QsC1kAkjzCrV"
   },
   "outputs": [],
   "source": [
    "# Cost comparison calculator\n",
    "def compare_deployment_costs(num_images_per_day, days_per_month=30):\n",
    "    \"\"\"\n",
    "    Compare costs between API and self-hosted deployment.\n",
    "    \"\"\"\n",
    "    total_images = num_images_per_day * days_per_month\n",
    "\n",
    "    # API costs (approximate for GPT-4V as of 2024)\n",
    "    api_cost_per_image = 0.01  # $0.01 per image (approximate)\n",
    "    api_total = api_cost_per_image * total_images\n",
    "\n",
    "    # Self-hosted costs\n",
    "    gpu_instance_per_month = 500  # $500/month for GPU instance\n",
    "    development_cost = 5000  # One-time setup cost\n",
    "    maintenance_per_month = 1000  # Engineering time\n",
    "\n",
    "    self_hosted_monthly = gpu_instance_per_month + maintenance_per_month\n",
    "    self_hosted_first_month = self_hosted_monthly + development_cost\n",
    "\n",
    "    # Analysis\n",
    "    print(\"=== Cost Comparison ===\")\n",
    "    print(f\"\\nScenario: {num_images_per_day:,} images/day = {total_images:,} images/month\\n\")\n",
    "\n",
    "    print(f\"üì± API Service (e.g., GPT-4V):\")\n",
    "    print(f\"   Monthly cost: ${api_total:,.2f}\")\n",
    "    print(f\"   Pros: No infrastructure, instant scaling, always up-to-date\")\n",
    "    print(f\"   Cons: Usage-based pricing, data privacy concerns\\n\")\n",
    "\n",
    "    print(f\"üñ•Ô∏è  Self-Hosted (e.g., BLIP on your server):\")\n",
    "    print(f\"   First month: ${self_hosted_first_month:,.2f} (includes setup)\")\n",
    "    print(f\"   Ongoing monthly: ${self_hosted_monthly:,.2f}\")\n",
    "    print(f\"   Pros: Data privacy, customization, fixed costs\")\n",
    "    print(f\"   Cons: Infrastructure management, maintenance burden\\n\")\n",
    "\n",
    "    # Break-even analysis\n",
    "    breakeven_months = (self_hosted_first_month - api_total) / (api_total - self_hosted_monthly)\n",
    "    if breakeven_months > 0:\n",
    "        print(f\"üí° Break-even point: ~{breakeven_months:.1f} months\")\n",
    "        print(f\"   After {breakeven_months:.1f} months, self-hosted becomes cheaper.\")\n",
    "    else:\n",
    "        print(f\"üí° At this volume, API service is more cost-effective.\")\n",
    "\n",
    "# Try different scenarios\n",
    "print(\"\\n=== SCENARIO 1: Small Startup ===\")\n",
    "compare_deployment_costs(num_images_per_day=100)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n=== SCENARIO 2: Medium Business ===\")\n",
    "compare_deployment_costs(num_images_per_day=10000)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n=== SCENARIO 3: Large Enterprise ===\")\n",
    "compare_deployment_costs(num_images_per_day=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5Vszn_czCrV"
   },
   "source": [
    "### ‚úÖ Knowledge Check 6: Deployment Decisions\n",
    "\n",
    "**Question 1**: Based on the cost analysis above, when does self-hosting make sense vs. using an API service?\n",
    "\n",
    "**Your Answer**:\n",
    "Self-hosting makes sense when volume is high/predictable (fixed GPU costs can be amortized), when you need low latency/offline control, or when privacy/compliance prevents sending images to third parties. APIs are typically better for low/variable volume and fast iteration when you want to avoid MLOps and infrastructure burden.\n",
    "\n",
    "**Question 2**: What other factors (beyond cost) should influence the API vs. self-hosted decision?\n",
    "\n",
    "**Your Answer**:\n",
    "Beyond cost: data privacy/regulatory constraints, latency and uptime requirements, customization/fine-tuning needs, security and auditing, vendor lock-in, reliability/SLA, and the team‚Äôs ability to operate GPU infrastructure.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNRyOLmmzCrV"
   },
   "source": [
    "## Section 6: Critical Analysis - Limitations and Ethics\n",
    "\n",
    "From the lecture, we learned about critical limitations and ethical considerations. Let's explore these hands-on.\n",
    "\n",
    "### Limitation 1: Hallucinations and Confabulations\n",
    "\n",
    "VLMs can generate plausible but incorrect descriptions. Let's test this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E5CnrLmnzCrV"
   },
   "outputs": [],
   "source": [
    "# Test for hallucinations\n",
    "print(\"=== Testing for Hallucinations ===\")\n",
    "print(\"\\nLet's ask questions that require counting or reading small text...\\n\")\n",
    "\n",
    "# Choose an image\n",
    "test_image = product_images[2] if len(product_images) > 2 else product_images[0]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(test_image)\n",
    "plt.axis('off')\n",
    "plt.title(\"Test Image\")\n",
    "plt.show()\n",
    "\n",
    "# Test with challenging questions\n",
    "if 'blip_vqa_model' in dir():\n",
    "    challenging_questions = [\n",
    "        \"How many objects are in this image?\",\n",
    "        \"What is the exact text written in the image?\",\n",
    "        \"What is the person thinking?\",\n",
    "        \"What happened right before this photo was taken?\"\n",
    "    ]\n",
    "\n",
    "    print(\"Asking challenging questions...\\n\")\n",
    "    for q in challenging_questions:\n",
    "        answer = answer_question_blip(test_image, q)\n",
    "        print(f\"Q: {q}\")\n",
    "        print(f\"A: {answer}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è VQA model not loaded. Here's what might happen:\\n\")\n",
    "    print(\"Q: How many people are in this image?\")\n",
    "    print(\"A: three  [But there might be 2 or 4!]\\n\")\n",
    "    print(\"Q: What is written on the sign?\")\n",
    "    print(\"A: stop  [But the text is too small to read!]\\n\")\n",
    "\n",
    "print(\"\\nüí° Observation:\")\n",
    "print(\"VLMs often provide confident answers even when they can't actually\")\n",
    "print(\"perform the task (like counting or reading small text).\")\n",
    "print(\"This is called 'hallucination' - generating plausible but incorrect information.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRVollpkzCrV"
   },
   "source": [
    "### ü§î Reflection Question 7: Hallucinations and Trust\n",
    "\n",
    "**Question**:\n",
    "1. Why are hallucinations particularly problematic for VLMs in high-stakes applications (medical, legal, safety)?\n",
    "2. The lecture mentioned that \"confidence doesn't correlate with accuracy.\" What does this mean, and why is it dangerous?\n",
    "3. How would you design a system to detect or mitigate hallucinations in production?\n",
    "4. What responsibility do AI developers have to communicate these limitations to users?\n",
    "\n",
    "**Your Answer**:\n",
    "1) In high-stakes settings, a confident but wrong answer can cause real harm (misdiagnosis, wrongful legal conclusions, safety incidents). VLM hallucinations are especially dangerous because outputs can sound authoritative even when unsupported by the image.\n",
    "2) ‚ÄúConfidence doesn‚Äôt correlate with accuracy‚Äù means the model‚Äôs tone/probability-like wording is not a reliable indicator of correctness. This is dangerous because users may over-trust fluent answers and skip verification.\n",
    "3) Mitigations: force grounding in approved sources (RAG over curated databases), implement abstention/deferral policies, add verification checks (consistency prompts, cross-model agreement, rule-based constraints for counts/units), log and review failures, and route uncertain/high-risk cases to humans.\n",
    "4) Developers must communicate limitations clearly (UI warnings, documentation, known failure modes), design for safe use (human-in-the-loop), and avoid deploying the system where the expected harms outweigh benefits.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-IcfM8XIzCrV"
   },
   "source": [
    "### Limitation 2: Bias and Fairness\n",
    "\n",
    "VLMs can perpetuate biases present in their training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NtPVWObzCrV"
   },
   "outputs": [],
   "source": [
    "# Conceptual exploration of bias\n",
    "print(\"=== Understanding Bias in VLMs ===\")\n",
    "print(\"\"\"\n",
    "VLMs are trained on internet data, which contains societal biases.\n",
    "\n",
    "Examples of potential biases:\n",
    "1. Gender associations\n",
    "   - \"doctor\" -> often associated with male images\n",
    "   - \"nurse\" -> often associated with female images\n",
    "\n",
    "2. Racial and cultural biases\n",
    "   - Facial recognition works better on some demographics\n",
    "   - Cultural contexts may be misunderstood\n",
    "\n",
    "3. Socioeconomic biases\n",
    "   - Certain lifestyles or settings over-represented\n",
    "   - Professional settings may be stereotyped\n",
    "\n",
    "4. Geographic biases\n",
    "   - Western-centric training data\n",
    "   - Other cultures under-represented\n",
    "\n",
    "üí° Why this matters:\n",
    "- Automated systems can perpetuate discrimination\n",
    "- Affects real people's lives (hiring, lending, justice)\n",
    "- Erodes trust in AI systems\n",
    "- Legal and ethical implications\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n=== Testing for Bias (Conceptual) ===\")\n",
    "print(\"\"\"\n",
    "To test for bias, you would:\n",
    "1. Create a diverse test set with balanced demographics\n",
    "2. Measure performance across different groups\n",
    "3. Look for disparities in accuracy, caption quality, etc.\n",
    "4. Analyze associations (what words/concepts cluster together)\n",
    "\n",
    "Example test:\n",
    "- Show images of doctors of different genders/races\n",
    "- Check if the model consistently identifies them as \"doctor\"\n",
    "- Or does it use different terms based on appearance?\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2ruDWTbzCrV"
   },
   "source": [
    "### ü§î Reflection Question 8: Ethics and Responsibility\n",
    "\n",
    "Recall the lecture's **Ethical Imperatives**:\n",
    "- üîí **Privacy**: Images contain sensitive information\n",
    "- üîç **Transparency**: Users must understand what's being analyzed\n",
    "- ‚öñÔ∏è **Fairness**: Test across diverse demographics\n",
    "- üë§ **Accountability**: Human oversight for high-stakes decisions\n",
    "\n",
    "**Question**: Consider a company deploying a VLM for automated resume screening that analyzes candidate photos.\n",
    "\n",
    "1. What ethical concerns does this raise?\n",
    "2. What could go wrong? (Think about bias, privacy, transparency)\n",
    "3. What safeguards would you require before deploying such a system?\n",
    "4. Should some applications of VLMs be off-limits entirely? Which ones and why?\n",
    "5. As an AI practitioner, what is your responsibility when asked to build such a system?\n",
    "\n",
    "**Your Answer**:\n",
    "1) Ethical concerns: discrimination and disparate impact (inferring race, gender, age, disability), privacy and consent issues (processing photos for hiring), lack of transparency, and potential violation of employment and civil-rights norms/laws.\n",
    "2) What could go wrong: biased rejection of qualified candidates, ‚Äúlookism,‚Äù reinforcing historical inequities, candidates not knowing how they were evaluated, data leakage/misuse, and significant legal/reputational risk.\n",
    "3) Safeguards: do not use candidate photos for screening; if images exist, strip them before modeling. Require documented purpose limitation, consent, fairness testing across demographics, independent audits, explainable decision criteria (based on job-relevant signals), and mandatory human review with appeal mechanisms.\n",
    "4) Some uses should be off-limits‚Äîe.g., automated hiring decisions based on appearance, mass surveillance for identifying ‚Äúsuspicious‚Äù people, and other applications where visual inference drives rights-impacting outcomes without due process.\n",
    "5) Practitioner responsibility: raise risks early, recommend safer alternatives (job-relevant text/skills-based screening), ensure compliance and auditing, and refuse to build or deploy systems that are inherently discriminatory or cannot be made safe.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mbIL3eIzCrV"
   },
   "source": [
    "### Limitation 3: Computational Costs and Environmental Impact\n",
    "\n",
    "From the lecture: Training costs $50K - $10M, inference takes 1-5 seconds, significant carbon footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDmVSwPWzCrV"
   },
   "outputs": [],
   "source": [
    "# Estimate computational costs\n",
    "import time\n",
    "\n",
    "print(\"=== Computational Cost Analysis ===\")\n",
    "\n",
    "# Measure inference time\n",
    "test_img = product_images[0]\n",
    "\n",
    "if 'clip_model' in dir():\n",
    "    # CLIP inference\n",
    "    categories = [\"a photo of a dog\", \"a photo of a cat\", \"a photo of a person\"]\n",
    "\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        _ = classify_image_clip(test_img, categories)\n",
    "    end = time.time()\n",
    "\n",
    "    avg_time = (end - start) / 10\n",
    "    print(f\"\\nCLIP inference time: {avg_time*1000:.1f} ms per image\")\n",
    "    print(f\"Throughput: {1/avg_time:.1f} images/second\")\n",
    "    print(f\"For 1M images: ~{avg_time*1e6/3600:.1f} hours\")\n",
    "\n",
    "elif 'blip_caption_model' in dir():\n",
    "    # BLIP inference\n",
    "    start = time.time()\n",
    "    for _ in range(5):\n",
    "        _ = generate_caption_blip(test_img)\n",
    "    end = time.time()\n",
    "\n",
    "    avg_time = (end - start) / 5\n",
    "    print(f\"\\nBLIP inference time: {avg_time*1000:.1f} ms per image\")\n",
    "    print(f\"Throughput: {1/avg_time:.1f} images/second\")\n",
    "    print(f\"For 1M images: ~{avg_time*1e6/3600:.1f} hours\")\n",
    "\n",
    "print(\"\\nüí° Implications:\")\n",
    "print(\"- Inference cost scales linearly with usage\")\n",
    "print(\"- GPU usage -> electricity consumption -> carbon emissions\")\n",
    "print(\"- Training is FAR more expensive (thousands of GPU-hours)\")\n",
    "print(\"\\nSustainability considerations:\")\n",
    "print(\"- Use smaller models when possible\")\n",
    "print(\"- Batch processing for efficiency\")\n",
    "print(\"- Consider carbon-aware computing (run during low-carbon electricity periods)\")\n",
    "print(\"- Model distillation (smaller model learns from larger one)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEu9QbZszCrV"
   },
   "source": [
    "### ‚úÖ Knowledge Check 7: Environmental Responsibility\n",
    "\n",
    "**Question**: A company wants to process every user-uploaded image through GPT-4V for content analysis. They process 100M images/month. What environmental and cost considerations should they think about? What alternatives might be more sustainable?\n",
    "\n",
    "**Your Answer**:\n",
    "They should consider energy use, GPU/CPU time, storage and bandwidth, carbon footprint, and the downstream impact of scaling (hardware procurement, cooling, data center emissions). Processing 100M images/month can be extremely costly environmentally and financially.\n",
    "\n",
    "More sustainable alternatives include: processing only when needed (on-demand), downsampling/compressing images, batching and caching embeddings, using smaller/distilled models, doing lightweight filtering on-device/edge, reusing precomputed features, and scheduling heavy jobs during lower-carbon energy periods or using data centers powered by renewable energy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhlfSYapzCrW"
   },
   "source": [
    "## Section 7: Synthesis and Final Reflection\n",
    "\n",
    "Congratulations on making it through the lab! Let's consolidate your learning.\n",
    "\n",
    "### üéØ Learning Outcomes Review\n",
    "\n",
    "Let's check if we've met our learning outcomes:\n",
    "\n",
    "1. ‚úÖ **Understand VLMs and Their Architectures**\n",
    "   - You explored CLIP's contrastive learning approach\n",
    "   - Or BLIP's generation capabilities\n",
    "   - You understood embeddings and shared representation spaces\n",
    "   - You learned about different bridging strategies (adapters vs. projectors)\n",
    "\n",
    "2. ‚úÖ **Understand Applications of VLMs**\n",
    "   - You implemented zero-shot classification and image search\n",
    "   - Or caption generation and VQA\n",
    "   - You explored real-world use cases\n",
    "   - You built a mini product search system\n",
    "\n",
    "3. ‚úÖ **Evaluate and Apply**\n",
    "   - You analyzed architectural trade-offs\n",
    "   - You compared API vs. self-hosted deployment\n",
    "   - You evaluated metrics and performance\n",
    "   - You considered ethical implications and limitations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9rrWUUQzCrW"
   },
   "source": [
    "### ü§î Final Reflection Questions\n",
    "\n",
    "These questions require you to synthesize everything you've learned. **Take 15-20 minutes to think and write thoughtful answers.**\n",
    "\n",
    "#### Question 1: Architecture Selection Framework\n",
    "\n",
    "Create a decision framework for choosing a VLM architecture. Fill in the table:\n",
    "\n",
    "| If you need... | Consider... | Because... |\n",
    "|---------------|-------------|------------|\n",
    "| Zero-shot classification | CLIP-style contrastive (image + text embeddings) | Enables open-vocabulary labeling via similarity without task-specific training |\n",
    "| Generate natural descriptions | BLIP/encoder‚Äìdecoder captioning (image ‚Üí text generation) | Produces fluent natural-language descriptions conditioned on the image |\n",
    "| Conversational interaction | Multimodal LLM (LLaVA-style) + retrieval (RAG) | Supports multi-turn Q&A and instruction following while grounding in trusted content |\n",
    "| Few-shot learning | Multimodal LLM with in-context examples / prompt tuning | Adapts to new tasks from a handful of examples without full retraining |\n",
    "| Complex reasoning | Multimodal LLM + tools (search/DB) + RAG | Reasoning needs planning and evidence; tools/RAG reduce hallucination and improve accuracy |\n",
    "**Your Framework**:\n",
    "(Completed in the table above.)\n",
    "\n",
    "---\n",
    "\n",
    "#### Question 2: Real-World Design Challenge\n",
    "\n",
    "**Scenario**: A national museum wants to create an interactive mobile app where visitors can:\n",
    "- Point their camera at any exhibit\n",
    "- Get a detailed description\n",
    "- Ask questions about the artwork\n",
    "- Hear stories and historical context\n",
    "\n",
    "**Your task**: Design the system. Address:\n",
    "1. Which VLM architecture(s) would you use? Why?\n",
    "2. API service or self-hosted? Justify your choice.\n",
    "3. What data would you need? How would you acquire it?\n",
    "4. What evaluation metrics matter for this use case?\n",
    "5. What could go wrong? How would you mitigate risks?\n",
    "6. How would you ensure accessibility and inclusivity?\n",
    "\n",
    "**Your Design**:\n",
    "**1) Architecture**: Use a hybrid approach:\n",
    "- **Recognition/Retrieval layer**: CLIP-style image embeddings to identify the exhibit (nearest-neighbor search) and pull curated metadata quickly.\n",
    "- **Conversational layer**: a multimodal instruction-tuned model (LLaVA-style / multimodal LLM) that answers questions and generates narratives, grounded via **RAG** over the museum‚Äôs approved catalog (artist, date, medium, provenance, curator notes, audio scripts).\n",
    "\n",
    "**2) API vs. self-hosted**: Hybrid deployment.\n",
    "- **Self-host** the embedding index + lightweight vision model for fast, predictable exhibit identification.\n",
    "- Use an **API** (or a centrally hosted multimodal LLM) for dialogue and storytelling to reduce ops burden and enable rapid iteration. If network/privacy constraints require it, host the LLM on museum-managed infrastructure.\n",
    "\n",
    "**3) Data needed / acquisition**:\n",
    "- High-quality images of each exhibit (multiple angles/lighting), plus a structured catalog (title, artist, year, medium, location).\n",
    "- Curator-approved descriptions, stories, and FAQs; multilingual versions; accessibility scripts.\n",
    "- Collect from the museum CMS/archives, digitization workflows, and curated Q&A written by staff; verify rights/permissions.\n",
    "\n",
    "**4) Evaluation metrics**:\n",
    "- **Retrieval**: Recall@1/5 for correct exhibit identification; latency and failure rate.\n",
    "- **Q&A/storytelling**: factuality/grounding rate (human review + spot checks), user satisfaction, engagement (time in experience), and safety incidents.\n",
    "- **Accessibility**: comprehension ratings, screen-reader compatibility, and user feedback from diverse audiences.\n",
    "\n",
    "**5) Risks / mitigations**:\n",
    "- Hallucinated facts ‚Üí enforce RAG grounding, show citations (‚ÄúFrom the museum catalog‚Äù), and allow abstention/escalation.\n",
    "- Misidentification ‚Üí confidence thresholds + ‚ÄúDid you mean‚Ä¶‚Äù alternatives and manual search fallback.\n",
    "- Privacy (visitor faces) ‚Üí process on-device when possible, avoid storing raw images, and minimize logging.\n",
    "- IP/rights issues ‚Üí use only licensed content and watermark/attribution policies.\n",
    "\n",
    "**6) Accessibility & inclusivity**:\n",
    "- Voice + text output, adjustable reading level (‚Äúkid mode‚Äù), large text/high contrast, multilingual support, and inclusive language guidelines.\n",
    "- Evaluate across accents/dialects and with users who have disabilities; provide a feedback mechanism to report issues.\n",
    "\n",
    "---\n",
    "\n",
    "#### Question 3: Comparing Your Path\n",
    "\n",
    "**If you did Path A (CLIP)**:\n",
    "- What are the limitations of CLIP compared to generative models like BLIP?\n",
    "- For what types of applications is CLIP's approach actually better?\n",
    "- How would you extend CLIP's capabilities without switching models?\n",
    "\n",
    "**If you did Path B (BLIP)**:\n",
    "- What are the trade-offs of generation vs. classification-only models?\n",
    "- When would CLIP's simpler approach be preferable to BLIP?\n",
    "- How do the computational costs compare?\n",
    "\n",
    "**Your Analysis**:\n",
    "**CLIP (Path A) vs. BLIP (Path B): key trade-offs**\n",
    "\n",
    "- **CLIP limitations vs. generative models**: CLIP does not natively generate language or handle open-ended Q&A; it mainly scores similarity between images and text. It can struggle with tasks that require detailed description, counting, or reading small text unless combined with additional components.\n",
    "- **Where CLIP is better**: high-throughput **retrieval**, open-vocabulary tagging, duplicate detection, and ‚Äúfind similar‚Äù search. It is often simpler, faster, and easier to deploy at scale (precompute image embeddings; do fast nearest-neighbor lookups).\n",
    "- **How to extend CLIP without switching models**: prompt engineering; add a small classifier head; fine-tune (or use adapters) on domain data; combine CLIP retrieval with a language model that generates answers from retrieved, curated text (RAG).\n",
    "\n",
    "- **BLIP trade-offs (generation vs. classification-only)**: BLIP can produce natural-language captions and support VQA, which is great for accessibility and descriptive tasks, but generation is typically slower and more compute-heavy, and it can hallucinate details.\n",
    "- **When CLIP‚Äôs simpler approach is preferable**: when you need low latency, high volume, predictable behavior, and ranking/retrieval is the core requirement.\n",
    "- **Compute comparison**: BLIP-style generation generally requires more GPU time per request than CLIP embedding + similarity, especially for multi-token outputs.\n",
    "\n",
    "---\n",
    "\n",
    "#### Question 4: The Future of VLMs\n",
    "\n",
    "Based on what you've learned:\n",
    "1. What do you think is the biggest remaining challenge for VLMs?\n",
    "2. What new applications of VLMs are you most excited about?\n",
    "3. What concerns you most about the widespread deployment of VLMs?\n",
    "4. If you were leading a VLM research team, what would you focus on?\n",
    "\n",
    "**Your Vision**:\n",
    "1) **Biggest remaining challenge**: reliable grounding and reasoning‚Äîreducing hallucinations, improving robustness to domain shift, and handling ‚Äúhard vision‚Äù skills (counting, OCR, fine-grained spatial relations) with calibrated uncertainty.\n",
    "2) **Most exciting applications**: accessibility assistants, education/tutoring with visual context, robotics/embodied agents, scientific/medical decision support (as assistive tools), and safer content moderation.\n",
    "3) **Biggest concerns**: surveillance and privacy abuse, unfair bias in high-stakes decisions, misinformation/over-trust due to fluent outputs, and the environmental cost of large-scale deployment.\n",
    "4) **Research focus**: grounding + uncertainty estimation, strong evaluation benchmarks that reflect real-world failure modes, privacy-preserving training/inference, and efficiency (distillation, sparse/quantized models) to reduce compute while maintaining quality.\n",
    "\n",
    "---\n",
    "\n",
    "#### Question 5: Ethical Leadership\n",
    "\n",
    "Recall the lecture's **Golden Rule**: \"VLMs are powerful assistants, NOT autonomous decision-makers.\"\n",
    "\n",
    "**Question**:\n",
    "1. Why is this distinction important?\n",
    "2. Give three examples of applications where VLMs should NOT be used autonomously.\n",
    "3. How would you design \"human-in-the-loop\" systems for high-stakes decisions?\n",
    "4. What training or guidelines would you give to people working with VLM outputs?\n",
    "\n",
    "**Your Answer**:\n",
    "1) The distinction matters because VLM outputs are not guaranteed to be correct; treating a model as an autonomous decision-maker invites automation bias, unaccountable harm, and unsafe deployment in high-stakes contexts.\n",
    "2) Examples that should not be autonomous: (a) medical diagnosis/treatment decisions, (b) legal judgments or eligibility determinations (benefits, immigration), (c) hiring/disciplinary decisions or policing/surveillance actions.\n",
    "3) Human-in-the-loop design: use the model for **triage and suggestions**, require source grounding, surface uncertainty, log outputs for audit, and mandate qualified human review/approval before any rights-impacting action. Provide an escalation path and allow the model to abstain.\n",
    "4) Training/guidelines for operators: treat outputs as hypotheses; verify with trusted sources; document decisions; understand known failure modes (hallucination, bias); never rely on model confidence alone; and follow privacy/security procedures for handling images.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4Edej0KzCrW"
   },
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### üéì What You've Accomplished\n",
    "\n",
    "In this lab, you:\n",
    "- ‚úÖ Understood how VLMs create shared embedding spaces for vision and language\n",
    "- ‚úÖ Implemented zero-shot classification and image search (CLIP) OR caption generation and VQA (BLIP)\n",
    "- ‚úÖ Explored model architectures and learned about fine-tuning concepts\n",
    "- ‚úÖ Evaluated VLM performance using appropriate metrics\n",
    "- ‚úÖ Analyzed real-world applications and deployment trade-offs\n",
    "- ‚úÖ Critically examined limitations, biases, and ethical considerations\n",
    "- ‚úÖ Synthesized knowledge to make informed architectural decisions\n",
    "\n",
    "### üìö Additional Resources\n",
    "\n",
    "To deepen your understanding:\n",
    "\n",
    "**Papers to Read**:\n",
    "- [CLIP Paper](https://arxiv.org/abs/2103.00020): \"Learning Transferable Visual Models From Natural Language Supervision\"\n",
    "- [BLIP Paper](https://arxiv.org/abs/2201.12086): \"BLIP: Bootstrapping Language-Image Pre-training\"\n",
    "- [LLaVA Paper](https://arxiv.org/abs/2304.08485): \"Visual Instruction Tuning\"\n",
    "\n",
    "**Interactive Demos**:\n",
    "- [Hugging Face Spaces](https://huggingface.co/spaces): Try CLIP, BLIP, and LLaVA demos\n",
    "- [OpenAI CLIP](https://openai.com/research/clip): Official CLIP demo\n",
    "\n",
    "**Datasets to Explore**:\n",
    "- [COCO Dataset](https://cocodataset.org/): Image captioning and detection\n",
    "- [Flickr30k](http://hockenmaier.cs.illinois.edu/Flickr30kEntities/): Image-caption pairs\n",
    "- [Visual Genome](https://visualgenome.org/): Detailed scene understanding\n",
    "\n",
    "**Practice Projects**:\n",
    "1. Build a visual search engine for your photo library\n",
    "2. Create an accessibility tool that describes images for screen readers\n",
    "3. Fine-tune CLIP on a specific domain (e.g., medical images, satellite imagery)\n",
    "4. Implement a bias detection tool for VLM outputs\n",
    "\n",
    "### üöÄ Challenge: Extend This Lab\n",
    "\n",
    "Try one of these extensions:\n",
    "1. **Implement the other path**: If you did Path A, try Path B (or vice versa)\n",
    "2. **Add data augmentation**: Improve robustness by testing with modified images\n",
    "3. **Build a GUI**: Create a simple web interface using Gradio or Streamlit\n",
    "4. **Conduct a bias audit**: Test your model on diverse images and measure fairness metrics\n",
    "5. **Implement caching**: Speed up repeated queries with embedding caching\n",
    "\n",
    "---\n",
    "\n",
    "### üôè Thank You!\n",
    "\n",
    "You've completed a comprehensive exploration of Visual Language Models. Remember:\n",
    "\n",
    "- **VLMs are powerful tools**, but they're not magic - understand their capabilities AND limitations\n",
    "- **Always evaluate critically** - high confidence doesn't mean high accuracy\n",
    "- **Consider ethics first** - your decisions as an AI practitioner affect real people\n",
    "- **Keep learning** - this field evolves rapidly!\n",
    "\n",
    "**Questions or feedback?** Discuss with your instructor and classmates.\n",
    "\n",
    "---\n",
    "\n",
    "*Lab by Patricia McManus for ITAI 1378 - Module 08: Visual Language Models*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
